---
title: "Modelos de regresión dinámica"
author: "Pablo Benavides-Herrera"
date: "2020-05-04"
output:
  html_notebook:
    toc: yes
    toc_float: yes
    theme: cerulean
    highlight: tango
    number_sections: TRUE
  github_document:
    toc: yes
    dev: jpeg
---

```{r pkgs, message=FALSE}
library(easypackages)
libraries("tidyverse","fpp3","tsibble","feasts","fable")
```


# Introducción

Los modelos de pronóstico de suavización exponencial y ARIMAs son estimados **a través de observaciones pasadas**, pero **no permiten incluir información exógena** a la serie (otras variables).

Por ejemplo, para pronosticar la demanda de energía eléctrica, podemos implementar un ARIMA estacional. Sin embargo, cuánta energía se consume en los hogares se ve afectada fuertemente por la temperatura ambiental en ese momento. Con el SARIMA solo podríamos obtener la dinámica de la propia serie, pero tal vez sería bueno incluir también como predictora a la temperatura.

Recordando, un modelo de regresión tiene la forma general

$$y_{t}=\beta_{0}+\beta_{1} x_{1, t}+\cdots+\beta_{k} x_{k, t}+\varepsilon_{t}$$

donde $y_t$ es la variable que queremos pronosticar, $x_{k, t}$ son las variables independientes que utilizábamos para explicar a $y_t$ y $\varepsilon_{t}$ es el término de error no correlacionado (ruido blanco).

Para extender ese modelo, ahora permitiremos que el término de error sí esté autocorrelacionado, por lo que lo sustituimos por $\eta_t$, que asumimos que sigue un proceso ARIMA:

$$\begin{array}{c}
y_{t}=\beta_{0}+\beta_{1} x_{1, t}+\cdots+\beta_{k} x_{k, t}+\eta_{t} \\
\left(1-\phi_{1} B\right)(1-B) \eta_{t}=\left(1+\theta_{1} B\right) \varepsilon_{t}
\end{array}$$

Este modelo tiene dos términos de error:  $\eta_t$ (el error de la regresión) y $\varepsilon_{t}$ (el error del proceso ARIMA). Sólo $\varepsilon_{t}$ se asume que es ruido blanco.

**NOTA:** Cuando se quiere realizar un modelo de regresión dinámica, es necesario que **todas las variables sean estacionarias**, por lo que primero se debe verificar que se cumpla eso, o que se conviertan en estacionarias. De hecho, si una variable requiere primeras diferencias, es conveniente aplicar las primeras diferencias a todas las variables. A esto se le conoce como un *modelo en diferencias*. A un modelo que toma los datos originales se le conoce como *modelo en niveles*.

# Regresión con errores ARIMA en **R** con `fable`

Podemos estimar un modelo de regresion que incluya errores ARIMA a través de la misma función utilizada antes, `ARIMA`. Si definimos el argumento especial `pdq()` con primeras diferencias (`pdq(d=1)`), **R** aplicará las primeras diferencias a todas las variables.

Para incluir a las variables independientes, basta con agregarlas del lado derecho de la fórmula. P. ej:

```
ARIMA(y ~ x + pdq(1,1,0))
```
estimará un modelo en diferencias $y_{t}^{\prime}=\beta_{1} x_{t}^{\prime}+\eta_{t}^{\prime}$, donde $\eta_{t}^{\prime}=\phi_{1} \eta_{t-1}^{\prime}+\varepsilon_{t}$ es un error que sigue un proceso AR(1).

Adicionalmente, la función `ARIMA()` puede encontrar de manera automática el orden del modelo, al simplemente **no especificar el argumento especial `pdq()`**.

## Ejemplo: Consumo personal e ingreso en EEUU

Se pretende analizar y pronosticar los cambios en el consumo personal a través de el ingreso disponible, utilizando datos de 1970 a 2016.

Cargamos los datos:
```{r us_change data}
us_change <- read_csv("https://otexts.com/fpp3/extrafiles/us_change.csv") %>%
  mutate(Time = yearquarter(Time)) %>%
  as_tsibble(index = Time)
```

Graficamos ambas series:

```{r us_change plot}
us_change %>%
  gather("var", "value", Consumption, Income) %>%
  ggplot(aes(x = Time, y = value)) +
  geom_line() +
  facet_grid(vars(var), scales = "free_y") +
  xlab("Year") + ylab(NULL) +
  ggtitle("Quarterly changes in US consumption and personal income")

us_change %>% 
  ggplot(aes(x = Income, y = Consumption))+
  geom_point()
```

Las series se ven estacionarias a simple vista. Ajustamos un modelo permitiendo errores ARIMA (calculados automáticamente):

```{r us_change model}
fit <- us_change %>%
  model(ARIMA(Consumption ~ Income))
report(fit)
```

El modelo ajustado tiene entonces la forma:

$$\begin{array}{l}
y_{t}=0.599+0.203 x_{t}+\eta_{t} \\
\eta_{t}=0.692 \eta_{t-1}+\varepsilon_{t}-0.576 \varepsilon_{t-1}+0.198 \varepsilon_{t-2} \\
\varepsilon_{t} \sim \mathrm{NID}(0,0.322)\\
y_{t}=0.599+0.203 x_{t} + 0.692 \eta_{t-1}+\varepsilon_{t}-0.576 \varepsilon_{t-1}+0.198 \varepsilon_{t-2} 
\end{array}$$

Podemos obtener los estimadores de las series $\eta_t$ y $\varepsilon_{t}$ con la función `residuals()`, especificando el tipo como `type = "regression"` para los errores de la regresión y `type = "innovations"` para los errores ARIMA.

```{r us_change residuals plot}
bind_rows(
  `Regression Errors` = as_tibble(residuals(fit, type="regression")),
  `ARIMA Errors` = as_tibble(residuals(fit, type="innovation")),
  .id = "type"
) %>%
  ggplot(aes(x = Time, y = .resid)) +
  geom_line() +
  facet_grid(vars(type), scales = "free_y") +
  xlab("Year") + ylab(NULL)
```

Solo debemos asegurarnos de que los errores ARIMA sean ruido blanco:

```{r us_change arima errors diagnostics}
fit %>% gg_tsresiduals()
```
La prueba de Ljung-Box:

```{r us_change ljung-box}
augment(fit) %>%
  features(.resid, ljung_box, dof = 5, lag = 8)
```

# Pronóstico

Para llevar a cabo pronósticos de modelos de regresión con errores ARIMA, se necesita realizar el pronóstico de

* la parte de la regresión
* la parte de los errores ARIMA

y combinar los resultados.

Una característica con estos modelos, es que necesitamos pronósticos de las variables independientes $x_t$ o predictoras para poder pronosticar nuestra variable de interés, $y_t$. Cuando las predictoras son conocidas en el futuro, como variables de calendario (tiempo, día de la semana, mes, etc.), no hay mayor problema. Pero, cuando son desconocidas, tenemos que o modelarlas por separado, o asumir valores futuros para cada una.

## Continuación ejemplo: Consumo personal e ingreso en EEUU

Obtenemos pronósticos para los siguientes dos años (8 trimestres), asumiendo que los cambios porcentuales en el ingreso serán iguales a el cambio promedio porcentual de los últimos 40 años:

```{r us_change fcst}
us_change_future <- new_data(us_change, 8) %>% mutate(Income = mean(us_change$Income))
forecast(fit, new_data = us_change_future) %>%
  autoplot(slice_tail(us_change, n = 80)) + 
  xlab("Year") +
  ylab("Percentage change") + 
  ggtitle("Pronóstico de regresión con errores ARIMA")
```

Cuando vimos los modelos ARIMA no estacionales, habíamos analizado esta misma serie. Recordando, el pronóstico resultaba:

```{r}
fit_prev <- us_change %>%
  model(ARIMA(Consumption ~ PDQ(0,0,0)))

fit_prev %>% forecast(h=10) %>% 
  autoplot(slice(us_change, (n()-80):n())) + 
  ggtitle("Pronóstico con modelo ARIMA")
```

La principal diferencia entre ambas es que, con nuestro nuevo modelo, logramos capturar más información y, por lo tanto, los intervalos de predicción se reducen.

**NOTA:** Los intervalos de predicción de modelos de regresión (regresión lineal múltiple o modelos con errores ARIMA), no toman en cuenta la incertidumbre de las predictoras. Así, el modelo *asume* que esas predicciones son correctas. En otras palabras, los intervalos de predicción son condicionales al cumplimiento de los valores de las predictoras.

## Ejemplo: Demanda de energía 

La demanda de energía diaria se puede modelar como una función de la temperatura del ambiente. Cuando la temperatura es muy extrema (muy alta o muy baja), tiende a aumentar la demanda de energía (por el uso de aires acondicionados y calefacción, respectivamente).

Grafiquemos la energía demandada vs. la temperatura global de cada día para ver este efecto:

```{r elec v. temp plot, paged.print=FALSE}
vic_elec_daily <- vic_elec %>%
  filter(year(Time) == 2014) %>%
  index_by(Date = date(Time)) %>%
  summarise(
    Demand = sum(Demand)/1e3,
    Temperature = max(Temperature),
    Holiday = any(Holiday)
  ) %>%
  mutate(Day_Type = case_when(
    Holiday ~ "Holiday",
    wday(Date) %in% 2:6 ~ "Weekday",
    TRUE ~ "Weekend"
  ))

vic_elec_daily

vic_elec_daily %>%
  ggplot(aes(x=Temperature, y=Demand, colour=Day_Type)) +
    geom_point() +
    ylab("Electricity demand (GW)") +
    xlab("Maximum daily temperature")

vic_elec_daily %>% 
  mutate(
    dia_habil = Day_Type == "Weekday"
  )
```

Como era de esperarse, la gráfica parece tener una forma de *U*. Adicionalmente, vemos que la energía demandada es mayor entre semana, luego en fin de semana, y por último en días festivos. Esto nos lleva a determinar que la demanda de energía es una función de la temperatura y del tipo de día en que nos encontremos.

Ahora revisamos las gráficas de tiempo de ambas variables:

```{r elec time plots, echo= TRUE}
vic_elec_daily %>% 
  pivot_longer(cols = c(Demand, Temperature), names_to = "vars",
               values_to = "value") %>% 
  ggplot(aes(x = Date, y = value)) + 
  geom_line() + 
  facet_wrap(~ vars, ncol = 1, strip.position = "right", scales = "free")
```

Dado que la relación entre las variables es cuadrática, ajustaremos un modelo cuadrático con errores ARIMA. Adicionalmente, agregaremos una variable para indicar si el día fue hábil o no.

$$
y_t = \beta_0 + \beta_1x_{t,Temperatura} + \beta_2x_{t,Temperatura}^2 + \beta_3D_{Dia hábil} + \eta_t
$$

```{r elec model, echo=TRUE}
fit <- vic_elec_daily %>%
  model(ARIMA(Demand ~ Temperature + I(Temperature^2) + (Day_Type=="Weekday")))

report(fit)
```

Revisamos el ajuste del modelo a través de sus residuos:

```{r elec resid diagnostics}
fit %>% gg_tsresiduals()
augment(fit) %>%
  features(.resid, ljung_box, dof = 8, lag = 14)
```
Se ven heteroscedásticos, ya que la variación es mayor en enero y un poco en febrero que en otros meses. Adicionalmente, vemos que hay rezagos significativos y que la distribución es de colas largas. Esto puede tener un impacto negativo en los intervalos de predicción, pero la estimación puntual es válida.

Así, entonces pronosticaremos los siguientes 14 días. Para esto, necesitamos datos respecto a la temperatura futura y podemos hacer dos cosas:

* Conseguir los datos de algún pronóstico meteorológico y meterlos al modelo.

* Podemos pronosticar bajo un escenario. Asumiremos que la temperatura máxima se mantendrá constante a 26 grados.

```{r elec fcst}
vic_elec_future <- new_data(vic_elec_daily, 14) %>%
  mutate(
    Temperature = 26,
    Holiday = c(TRUE, rep(FALSE, 13)),
    Day_Type = case_when(
      Holiday ~ "Holiday",
      wday(Date) %in% 2:6 ~ "Weekday",
      TRUE ~ "Weekend"
    )
  )
forecast(fit, vic_elec_future) %>%
  autoplot(vic_elec_daily) + ylab("Electricity demand (GW)")
```

Utilizando un pronóstico de la temperatura máxima para los siguientes 14 días tendríamos:

```{r}
vic_elec_future <- new_data(vic_elec_daily, 14) %>%
  mutate(
    Temperature = c(seq(31,34),33,rep(34,3),rep(33,6)),
    Holiday = c(TRUE, rep(FALSE, 13)),
    Day_Type = case_when(
      Holiday ~ "Holiday",
      wday(Date) %in% 2:6 ~ "Weekday",
      TRUE ~ "Weekend"
    )
  )
forecast(fit, vic_elec_future) %>%
  autoplot(vic_elec_daily) + ylab("Electricity demand (GW)")
```


# Regresión armónica dinámica



$$
\frac{a_{0}}{2}+\sum_{n=1}^{\infty}\left[a_{n} \cos \frac{2 n \pi}{T} t+b_{n} \sin \frac{2 n \pi}{T} t\right]
$$

$$ 
\frac{a_{0}}{2} + a_{1} \cos \frac{2 \pi}{T} t+b_{1} \sin \frac{2 \pi}{T} + a_{2} \cos \frac{2 * 2 \pi}{T} t+b_{2} \sin \frac{2*2 \pi}{T} + a_{3} \cos \frac{2 * 3 \pi}{T} t+b_{3} \sin \frac{2*3 \pi}{T}
$$

Como habíamos mencionado anteriormente, utilizar series de Fourier para modelar periodos estacionales largos suele ser mejor que simplemente utilizar *dummies* estacionales.

Algunos ejemplos de estacionalidades largas son:

* Para datos diarios, donde pueden tener estacionalidad anual (un periodo estacional $m = 365$),
* Datos semanales ($m = 52$),
* Datos por cada media hora ($m = 48$)

Incluso, muchas de estas series pueden presentar más de una estacionalidad (estacionalidad anual y semanal, p. ej., o estacionalidad anual y mensual).

Los modelos ARIMA y ETS son buenos para periodos más cortos (mensuales, trimestrales). Para aquellos casos, sería mejor emplear una regresión armónica.

Algunas de las ventajas:

* Permite cualquier tamaño de estacionalidad
* Se pueden modelar varios tipos de estacionalidad simultáneamente, al incluir términos de Fourier de distintas frecuencias.
* La suavización del patrón estacional es controlado por $K$ (el número de pares de senos y cosenos de Fourier). *El patrón estacional es más suave con valores más pequeños de $K$*
* La dinámica de corto plazo puede ser controlada fácilmente a través de errores ARMA.

La mayor desventaja de estos modelos, en comparación a los SARIMA es que **la estacionalidad se asume fija**. Esto no es tan drástico en la práctica, salvo en series de tiempo muy largas.

## Ejemplo: Gasto en comidas fuera de casa

```{r}
aus_cafe <- aus_retail %>%
  filter(
    Industry == "Cafes, restaurants and takeaway food services",
    year(Month) %in% 2004:2018
  ) %>%
  summarise(Turnover = sum(Turnover))
aus_cafe
aus_cafe %>% autoplot()
```

Probemos con varios niveles de $K$ para ver las diferencias entre las series (desde $K=1$, hasta $K=6$, que éste último sería igual a utilizar dummies estacionales).

```{r}
fit <- aus_cafe %>%
  model(
    `K = 1` = ARIMA(log(Turnover) ~ fourier(K = 1) + PDQ(0,0,0)),
    `K = 2` = ARIMA(log(Turnover) ~ fourier(K = 2) + PDQ(0,0,0)),
    `K = 3` = ARIMA(log(Turnover) ~ fourier(K = 3) + PDQ(0,0,0)),
    `K = 4` = ARIMA(log(Turnover) ~ fourier(K = 4) + PDQ(0,0,0)),
    `K = 5` = ARIMA(log(Turnover) ~ fourier(K = 5) + PDQ(0,0,0)),
    `K = 6` = ARIMA(log(Turnover) ~ fourier(K = 6) + PDQ(0,0,0))
  )
glance(fit)
```

Veamos los modelos escogidos en cada caso:

```{r}
rep_model <- function(modelo) {
  print(strrep("- - ",18))
  print(paste("Modelo", modelo, sep = " "))
  fit %>% 
    select(all_of(modelo)) %>% 
    report()
}

names(fit) %>% 
  map(rep_model)
```

Grafiquemos los pronósticos resultantes de cada modelo:

```{r, fig.height=5, fig.width=8}
p <- fit %>%
  forecast(h = "2 years") %>%
  autoplot(aus_cafe) +
  facet_wrap(vars(.model), ncol = 2) +
  guides(colour = FALSE) +
  geom_label(
    aes(x = yearmonth("2007 Jan"), y = 4250, label = paste0("AICc = ", format(AICc))),
    data = glance(fit)
  )
p
```



```{r}
p <- fit %>% 
  select(`K = 3`) %>% 
  forecast(h = "2 years") %>%
  autoplot(aus_cafe) +
  facet_wrap(vars(.model), ncol = 2) +
  guides(colour = FALSE)
p
```


